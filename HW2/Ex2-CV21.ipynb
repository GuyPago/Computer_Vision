{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## <span style=\"color:blue\">Computer Vision - Fall 2021\n",
    "\n",
    "\n",
    "**Lecturer:** Prof. Yael Moses, IDC\n",
    "\n",
    "**TA:** Eyal Friedman, IDC\n",
    "\n",
    "**Sybmission date: 3.12.2020**\\\n",
    "Note: in case you need an extenstion for any reason - you can submit it by 7.12.21.\n",
    "No extra extensions will be given.\n",
    "\n",
    "## <span style=\"color:blue\">Exercise 2  </span>\n",
    "\n",
    "In this exercise you will practice working with geometric tools for analyzing 3D scenes from 2D images.\n",
    "In particular, computing projection matrices, epipolar geometry, and stereo matching.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "1. Your **zip** should include the following files only:\n",
    "    - ex2.ipynb  (**Or**  ex2.py for students who refuses to work with Jupiter Notebook). \n",
    "    - ex2_ID_ID.pdf (If you decieded not to answer on some of the questions in the notebook, you should submit it as a pdf file). \n",
    "   (Don't add the python code to that file.)\n",
    "4. You may use any IDE as you want (Spyder, Jupyter Notebook, Pycharm, ect.).\n",
    "5. Name the zip file **'ex2_ID_ID.zip'** and **do not** include any additional directories. \n",
    "6. Submit using *moodle*.\n",
    "7. Submit on time!\n",
    "8. You can submit this assignment in pairs (no triplets)\n",
    "- **Important - if you submit in pairs, one should submit the howework and the other should submit a simple text file named: ID_ID.txt and nothing else.**\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Do not submit your tests, unless requested.\n",
    "3. Use `python 3` and `numpy 1.18.5`. Changes of the configuration we provided are at your own risk. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works.\n",
    "4. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden, unless been provided by us.\n",
    "4. Your code must run without errors. Note,  **code that fails to  run will not be graded.**\n",
    "5. Document your code properly.\n",
    "\n",
    "## Honor Code:\n",
    "The assignment is a basic tool for learning the material. You can probably find the solution on the Web. However, if  you do so, then you will not learn what you should learn from it. In addition, since we  grade  the assignment, using existing solutions will be considered dishonest.\n",
    "In particular, you are not allowed to copy or use any code that solves the task. \n",
    "You are more than welcome to talk with your friends, but you are not allowed to give your code or answers and you are not allowed to use their code or answers. \n",
    "Remember â€“ you take this course in order to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# This opens an inteactive figure - use it in part B\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "# This specifies the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Projection\n",
    "\n",
    "**In this part you will go over projection matrix,  and use them to project 3D points to an image.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values, given partial values of the parameters of the left and right cameras.\n",
    "\n",
    "\\\n",
    "**Right image parameters:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection matrix of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "MR = np.array([[1100.504780,          0,   331.023000,   0],\n",
    "               [0,          1097.763735,   259.386377,   0],\n",
    "               [0,                    0,            1,   0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rotation matrix of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "RR = np.array([[1,0,0],\n",
    "               [0,1,0],\n",
    "               [0,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focal length of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fR = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, replace \"none\" with your answers to the questions. In addition, if there are more than a single possible solution, choose one.\n",
    "Compute the right image center (principal point):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OxR = None\n",
    "OyR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the right image scale factor which is consistent with MR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SxR = None\n",
    "SyR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the right image intrinsic matrix which is consistent with MR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MintR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Left image parameters**\n",
    " \n",
    "Left image center (principal point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OxL = 320.798101 \n",
    "OyL = 236.431326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SxL = 1095.671499\n",
    "SyL = 1094.559584 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal length of the left image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation vector w.r.t. the world origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL = -np.array([[178.2218,18.8171,-13.7744]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation matrix of the left image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = np.array([[ 0.9891,    0.0602,   -0.1346],\n",
    "               [-0.0590,    0.9982,    0.0134],\n",
    "               [0.1351,   -0.0053,    0.9908]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the intrinsic projection matrix of the left camera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MintL = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the projection matrix of the left camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the COP of the left and the right images, in Cartesian coordinates:   \n",
    "\n",
    "(You may use the the function *null_space* from *scipy.linalg*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL = None\n",
    "CR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distance between CL and CR:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A2: Hands on Triangulation \n",
    "\n",
    "Write a function p = proj(M,P) that recieves as input the 3D point P in Euclidean coordinates and a projection matrix M, and outputs the 2D  Euclidean coordinates of the projected point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for proj function\n",
    "def proj(M,P):\n",
    "    \n",
    "    # your code\n",
    "    pass\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tGiven object points in the world coordinate system,  P=(-140,50,1200) and Q=(30,100,2000).\n",
    "\n",
    "    a.\tWhat are the coordinates (Euclidean) of the points in the left camera coordinate system?\\\n",
    "    b.\tWhat are the coordinates (Euclidean) of the points in the right camera coordinate system?\n",
    "    \n",
    "    Note: the camera coordinate system rather then the image coordinate system (PL means the 3D coordinates in the left **camera** cordinates system, and pL means the 2D coordinates in the left **image** coordinates system.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PL = None\n",
    "PR = None\n",
    "QL = None\n",
    "QR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[-140],[50],[1200]])\n",
    "pL = proj(ML,P)\n",
    "pR = proj(MR,P)\n",
    "\n",
    "Q = np.array([[30],[100],[2000]]) \n",
    "qL = proj(ML,Q)\n",
    "qR = proj(MR,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read two images and display the projections of P and Q on the two given images ###\n",
    "\n",
    "[//]: # \" \"\n",
    "The code below should return this result: \n",
    "\n",
    "![Example](PandQprojections1.png \"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "ax1.imshow(imL, cmap='gray'), ax1.set_title('Left image'), ax1.scatter(pL[0], pL[1], color='r'), \\\n",
    "    ax1.scatter(qL[0],qL[1], color = 'b')\n",
    "ax2.imshow(imR, cmap='gray'), ax2.set_title('Right image'), ax2.scatter(pR[0], pR[1], color = 'r'), \\\n",
    "    ax2.scatter(qR[0],qR[1], color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question:**\\\n",
    "Look at the projection of each of the points in the two images. One pair looks as expected, and the other does not. Please give a short explanation of what may have caused it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Epipolar Geometry\n",
    "Compute the fundamental matrix F and the epipoles eL and eR of the left and right images, using their projection matrices.\n",
    "Note, you should normalize F by F(3,3) for improved precision.\n",
    "\n",
    "For the epipoles' computation use the MR and ML and the Center of projections.\n",
    "\n",
    "**Answer Quesion:**\n",
    "Can you double check if they are correct using F? If so, check it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eL = None\n",
    "eR = None\n",
    "F = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epipolar lines ##\n",
    "\n",
    "Click on three different points of the **right** image, and check if the epipolar lines on the left image pass through a pixel that corresponds to the one you picked in the right image. Output the set of epipolar lines overlayed on the pair of  images as shown below.\n",
    "\n",
    "To do so you can use:\n",
    "1. The code below opens the images in a seperate window. You can click on the right image and  capture the click's coordinates by using the function *plt.ginput*.\n",
    "2. Take each point (this can be done by a loop) and calculate its epipolar line  on the left image using F.\n",
    "3. Compute the two endpoints of the line in the image to plot it on the left image. \\\n",
    "    **Hint**: you have linear coefficients - (a,b,c). Calculate the y value in the image for x=0, and x=image.width and plot the result.\\\n",
    "    Use: ax2.plot((x0. xWidth),(yx0, yxWidth))\n",
    "4. Use the set of the points of the right image that you collected, and draw the epipolar lines on the right image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This sould open a new figure window outside of jupyter notebook\n",
    "%matplotlib qt  \n",
    "\n",
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "ax1.imshow(imL, cmap='gray'), ax1.set_title('Left image')\n",
    "ax2.imshow(imR, cmap='gray'), ax2.set_title('Right image')\n",
    "\n",
    "data = plt.ginput(3)\n",
    "\n",
    "x_val = [x[0] for x in data]\n",
    "y_val = [x[1] for x in data]\n",
    "\n",
    "ax2.scatter(x_val, y_val, color='r')\n",
    "\n",
    "for x in data: \n",
    "    # Write your own implementation here.\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what you should see:\n",
    "![Epipolar](epipolarLines1.png \"Epipolar Lines example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C : SIFT and RANSAC/LMedS\n",
    "**Follow the matching to compute F.**\n",
    "\n",
    "https://docs.opencv.org/master/da/de9/tutorial_py_epipolar_geometry.html \n",
    "\n",
    "The example attached here needs some twicks to make it work. First you need to uninstall the opencv package and to install to opencv-contrib package:\n",
    "- pip uninstall opencv-python \n",
    "\n",
    "Then install the contrib version with this:\n",
    "- pip install opencv-contrib-python\n",
    "\n",
    "**<span style=\"color:red\"> Now, you have to use those lines:**\n",
    "- **sift = cv2.xfeatures2d.SIFT_create()**\n",
    "- **kp1, des1 = sift.detectAndCompute(img1, None)**\n",
    "\n",
    "\n",
    "Hereby, we will find the corresponding featues using the SIFT algorithm and match the closet points. The plotted figure showes the best 300 matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "# In the link above you need to change the next line from cv.SIFT to cv2.xfeatures2d.\n",
    "# Instead of: sift = cv2.SIFT() use:\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(imL,None)\n",
    "kp2, des2 = sift.detectAndCompute(imR,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "# create FlannBasedMatcher object\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "matching = []\n",
    "# Building a list of points screened by ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "        matching.append(m)\n",
    "        \n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matching = sorted(matching, key = lambda x:x.distance)\n",
    "        \n",
    "# Draw first 300 matches.\n",
    "img3 = np.array([])\n",
    "img3 = cv2.drawMatches(imL, kp1, imR, kp2, matching[:300], outImg = img3, flags =2)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1)) = plt.subplots(1, 1, sharex='col', sharey='row')\n",
    "ax1.imshow(img3, cmap='gray'), ax1.set_title('Matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not for submission: Look at the obtain results.\n",
    "\n",
    "    a. Do you think all matches are correct?\n",
    "    b. In which regions of the scene, most of the reliable matches were found?\n",
    "    c. Tru the worst 200 mathces as well -- matching[-1-200:]\n",
    "\n",
    "Now, we will use the found matches to compute **F** using *cv2.findFundamentalMat()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "\n",
    "# Computing the F matrix\n",
    "F_calc, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_LMEDS)\n",
    "# We select only inlier points\n",
    "pts1 = pts1[mask.ravel()==1]\n",
    "pts2 = pts2[mask.ravel()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F_calc.T)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets check the computed F_calc:\n",
    "1. Use it to draw the epipolar line as in the example above (change F to F_calc.T)\n",
    "2. Compute the distance between the computed epipoles by F and by F_calc in each of the images.\n",
    "\n",
    "Hint: You can use scipy.linalg import null_space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your part in this section :) ###\n",
    "\n",
    "#### Take two images by your camera and compute the epipolar geometry using LMedS ####\n",
    "\n",
    "Please submit in the pdf file: 5 corresponding epipolar lines overlayed on   your pair of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part D1 ##\n",
    "\n",
    "In this part you will compute the 3D structure from a pair of rectified images, using three variants of a simple stereo  algorithm on rectified images, $A1$, $A2$, and $A3$ where:\n",
    "- **$A1$** is an algorithm that computes the max similarity between two patches (without any attention for order or order-preservation) using cross correlation.\n",
    "- **$A2$** is an algorithm that computes the max similarity (as $A2$), but with order-preserving.\n",
    "- **$A3$** is an algorithm that computes the max similarity with order-preserving (as $A2$) but now, instead of using the intensity, we will use the magnitude of the gradients (As in Ex1).\n",
    "\n",
    "\n",
    "**The input:**\\\n",
    "    **a.** A pair of two rectified images, $im1$ and $im2$ (*view1.png and view5.png*).\\\n",
    "    **b.** A window size $(s_x,s_y$).\\\n",
    "    **c.** Disparity range $(d_{min},d_{max})$ (see below).\n",
    "\n",
    "**The output:**\\\n",
    "**a.** A matrix, $D$ with the disparity map for the left image.\\\n",
    "**b.** Three matrices X, Y, Z with the x, y, z coordinates of each pixel in the left image. Assign zero for pixels for which the disparity was not computed.\n",
    "\n",
    "Note: all matrices $im1, im2, D, X, Y, Z$ have the same dimensions.\n",
    "\n",
    "**Instructions:**\n",
    "1. Read the two images view1.png and view5.png.\n",
    "   The image planes are co-planar. The distance between the cameras is 160mm.\n",
    "   \n",
    "2. Write an algorithm that receives two rectified images (coplanar and parallel to the line connecting the two COP, with the same focal length), and compute a naive disparity along corresponding epipolar lines, using similarity measure for matching.\n",
    " 1. The similarity of two $s_x\\times s_y$ patches should be computed using normalized cross correlation of their vector descriptors. Cross correlation of two vectors $v_1$ and $v_2$ is  defined by: $$\\frac{v_1â‹…v_2}{â€–v_1 â€–â€–v_2 â€–}.$$ \\\n",
    "    For example, the descriptor $v_1$ can be ab $3\\times 3$ patch around a pixel $(i,j)$ by reshape the $3\\times 3$ patch around the pixel to a vector. You can use the reshape function:  $$v_1 = im1[i-1:i+1,j-1:j+1].reshape(1,9)$$.\n",
    "    \n",
    "    Notes: \n",
    "    - For $A1$ and $A2$ versions of the algorithm,  the descriptor is defined to be the grey-level values of the patch. For $A3$ version, the descriptor is defined as the set of magnitudes of the gradients in each pixel in the patch.\n",
    "    - In case you use vectorized implementation, which is briefly discussed below, you do not need the reshape function. \n",
    "    \n",
    " 2. For a naive disparity (version $A1$ of the algorithm), find for each patch in $im1$ the patch with the highest similarity   along the corresponding epipolar line in $im2$.\\\n",
    "    For order-preserving computation of the disparity (versions $A2$ and $A3$ of the algorithm) choose the corresponding patched such the order of matching is preserved.\\\n",
    "Note: you can assume that you are given the range of disparities, $d_{min},d_{max}$ as an input. For example, if $(d_{min},d_{max})=(20,120)$ it follows that you search for $(x,y)$ in the left image, its corresponding patch in the range $(x-120:x-20,y)$.\n",
    "   \n",
    " 3. Apply your functions to view1.png and view5.png, and display the disparity $D$ map as an image. \n",
    "   \n",
    "3. Compute the depth map using the disparity. Add to your disparity depth map the value 100, since images were cropped. Note â€“ simple triangulation can be applied here. Display it as an image. Assume that scaled focal lengths ($f$ in the presentation) are $Î±_x=Î±_y=1$.\n",
    "\n",
    "4. Compute the matrices X,Y,Z - and present them as a 3D plot.\n",
    "\n",
    "\n",
    "**Implementation**\\\n",
    "You can implement the computation of the similarity and the disparity directly by an exhaustive search. However, it will take a long time to run. Full credit will be given to a vectorized solution, as was briefly discussed in class.\n",
    "\n",
    "General idea for the **vectorized solution** (you will need to fill in the gaps):\n",
    "* Let $w$ and $h$ we the width and the height of the image respectively.\n",
    "* First assume that the patch size is $1\\times s_y$.\n",
    "* Given two $1\\times w$ rows, $r_1$ and $r_2$ which are corresponding rows of the two images. Compute the $w\\times w$ matrix $R12=r_1^T r_2$. The values $R12(i,j)$ consists of the product of two pixels $r_1(i)\\cdot r_2(j)$.\n",
    "* Convolve $R12$ with the $s_y\\times s_y$ identity matrix (used as a convolution mask): $Corr=I_{s_y}*R12$. The value $Corr(i,j)$ is the correlation of the $s_y$ 1D neighborhood of $r_1(i)$ and  $r_2(j)$.\n",
    "* Think about how to normalize this correlation using the same idea. Hint, use the computation of $I_{s_y}*(r_1^Tr_1)$ and $I_{s_y}*(r_2^Tr_2)$.\n",
    "* For a general patch size, $s_x\\times s_y$, define $R$ to be 3D matrix of size $h\\times w\\times s_x$ and use 3D mask for the convolution.\n",
    "* Up to here, we considered each row seperately, hence we need to loop over all rows of the images. You can further improve the computation time by computing directly the $h\\times w\\times w$ matrix, $R_{all}$, for all rows simultanouesly. In this case $R_{all}(i,j,k)$ consists of $im1(i,j)\\cdot im2(i,k)$. See an example in ex2_example_vectorizing.ipynb of how to implement it in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D2 \n",
    "\n",
    "**Answer Questions:**\n",
    "1. Suggest a method for quantitative evaluation of your results. (You do not need to implement it.)\n",
    "2. What are the differences in the results of applying $A1$ and $A2$? Describe failure of one of them with respect to the other, if exists.\n",
    "3.  What are the differences in the results of applying $A2$ and $A3$? Describe failure of one of them with respect to the other, if exists.\n",
    "4. Which regions have more errors? Why?\n",
    "\n",
    "BONUS: repeat D1 but use dynamic programing to compute an optimal order preserving disparity. Note, the data is already ready for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
